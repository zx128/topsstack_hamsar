{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "patient-happiness",
   "metadata": {},
   "source": [
    "# TOPS Stack Notebook\n",
    "\n",
    "## Introduction\n",
    "In this notebook, we will run all steps of processing Tops Stack. \n",
    "The detailed algorithm for stack processing of TOPS data can be find here:\n",
    "\n",
    "+ Fattahi, H., P. Agram, and M. Simons (2016), A Network-Based Enhanced Spectral Diversity Approach for TOPS Time-Series Analysis, IEEE Transactions on Geoscience and Remote Sensing, 55(2), 777-786, doi:[10.1109/TGRS.2016.2614925](https://ieeexplore.ieee.org/abstract/document/7637021)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-spyware",
   "metadata": {},
   "source": [
    "### Define input parameters with default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-crossing",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parmeters for TOPS Stack processor\n",
    "\n",
    "min_lat = 35.57793709766442\n",
    "max_lat = 36.21619047354823\n",
    "min_lon = -84.70058767311058\n",
    "max_lon = -83.94773267597341\n",
    "\n",
    "bbox = \"\"\n",
    "\n",
    "master_date = \"\"\n",
    "\n",
    "start_date = \"2020-10-01\"\n",
    "end_date = \"2020-10-31\"\n",
    "\n",
    "track_number = 7\n",
    "\n",
    "# PCM-System Parameters\n",
    "# These use reserved-prefix parameter names (_*) and are also parsed during `notebook-pge-wrapper specs` to generate the hysds-io and job-spec\n",
    "_time_limit = 86400\n",
    "_soft_time_limit = 86400\n",
    "_disk_usage = \"100GB\"\n",
    "_submission_type = \"individual\"\n",
    "_required_queue = \"factotum-job_worker-small\"\n",
    "_label = \"topsStack Processor for HAMSAR\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-camping",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# if not already exists, create file _context.json with current parameters,\n",
    "# needed by several tools below\n",
    "d = {\n",
    "\"min_lat\": min_lat,\n",
    "\"max_lat\": max_lat,\n",
    "\"min_lon\": min_lon,\n",
    "\"max_lon\": max_lon,\n",
    "\"bbox\": [],\n",
    "\"master_date\": master_date,\n",
    "\"start_date\": start_date,\n",
    "\"end_date\": end_date,\n",
    "\"track_number\": track_number\n",
    "}\n",
    "\n",
    "path = \"./_context.json\"\n",
    "if os.path.exists(path):\n",
    "    pass\n",
    "else:\n",
    "    print(\"create file %s with current parameters\" % path)\n",
    "    with open(path, \"w\") as fout:\n",
    "        json.dump(d, fout, indent=4)\n",
    "\n",
    "# if not already exists, create empty file _stdout.txt,\n",
    "# required by ${toolDir}/07-create-datasets.sh\n",
    "path = \"./_stdout.txt\"\n",
    "if os.path.exists(path):\n",
    "    pass\n",
    "else:\n",
    "    print(\"create empty file %s\" % path)\n",
    "    with open(path, \"w\") as fout:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-administration",
   "metadata": {},
   "source": [
    "### Processing Timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-finding",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "toolDir=~/topsstack_hamsar/tool\n",
    "\n",
    "# initialize state file\n",
    "STATE_FILE_NAME=_state.json\n",
    "echo '{}' > ./${STATE_FILE_NAME}\n",
    "\n",
    "prefix=\"_x\"\n",
    "processingStart=$(date +%FT%T)\n",
    "python ${toolDir}/state_rw.py write ${prefix}_processing_start $processingStart\n",
    "echo \"Processing start\" $processingStart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-village",
   "metadata": {},
   "source": [
    "### Prepare scenes\n",
    "\n",
    "This step will downloads SLCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-mambo",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "toolDir=~/topsstack_hamsar/tool\n",
    "\n",
    "source ${toolDir}/prepare_scenes.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-owner",
   "metadata": {},
   "source": [
    "### Get SLCs\n",
    "\n",
    "This step gets SLCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-architect",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "toolDir=~/topsstack_hamsar/tool\n",
    "\n",
    "source ${toolDir}/00-init.sh\n",
    "source ${toolDir}/01-setup.sh\n",
    "\n",
    "sh ${toolDir}/02-get-slcs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-triangle",
   "metadata": {},
   "source": [
    "\n",
    "### Fetch DEM\n",
    "\n",
    "This step downloads DEMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-paper",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "toolDir=~/topsstack_hamsar/tool\n",
    "\n",
    "source ${toolDir}/00-init.sh\n",
    "source ${toolDir}/01-setup.sh\n",
    "\n",
    "sh ${toolDir}/03-get-dems.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-width",
   "metadata": {},
   "source": [
    "### Create run scripts and fetch orbits\n",
    "\n",
    "The scripts provides support for Sentinel-1 TOPS stack processing. Currently supported workflows include a coregistered stack of SLC, interferograms, offsets, and coherence. \n",
    "\n",
    "`stackSentinel.py` generates all configuration and run files required to be executed on a stack of Sentinel-1 TOPS data. When stackSentinel.py is executed for a given workflow (-W option) a **configs** and **run_files** folder is generated. No processing is performed at this stage. Within the run_files folder different run\\_#\\_description files are contained which are to be executed as shell scripts in the run number order. Each of these run scripts call specific configure files contained in the “configs” folder which call ISCE in a modular fashion. The configure and run files will change depending on the selected workflow. To make run_# files executable, change the file permission accordingly (e.g., `chmod +x run_01_unpack_slc`).\n",
    "\n",
    "```bash\n",
    "stackSentinel.py -H     #To see workflow examples,\n",
    "stackSentinel.py -h     #To get an overview of all the configurable parameters\n",
    "```\n",
    "\n",
    "Required parameters of stackSentinel.py include:\n",
    "\n",
    "```cfg\n",
    "-s SLC_DIRNAME          #A folder with downloaded Sentinel-1 SLC’s. \n",
    "-o ORBIT_DIRNAME        #A folder containing the Sentinel-1 orbits. Missing orbit files will be downloaded automatically\n",
    "-a AUX_DIRNAME          #A folder containing the Sentinel-1 Auxiliary files\n",
    "-d DEM_FILENAME         #A DEM (Digital Elevation Model) referenced to wgs84\n",
    "```\n",
    "\n",
    "In the following, different workflow examples are provided. Note that stackSentinel.py only generates the run and configure files. To perform the actual processing, the user will need to execute each run file in their numbered order.\n",
    "\n",
    "In all workflows, coregistration (-C option) can be done using only geometry (set option = geometry) or with geometry plus refined azimuth offsets through NESD (set option = NESD) approach, the latter being the default. For the NESD coregistrstion the user can control the ESD coherence threshold (-e option) and the number of overlap interferograms (-O) to be used in NESD estimation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-shift",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "toolDir=~/topsstack_hamsar/tool\n",
    "\n",
    "source ${toolDir}/00-init.sh\n",
    "source ${toolDir}/01-setup.sh\n",
    "\n",
    "sh ${toolDir}/04-make-run-scripts.sh\n",
    "sh ${toolDir}/05-make-run-scripts.sh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-photographer",
   "metadata": {},
   "source": [
    "### Unpack TOPO reference\n",
    "\n",
    "Unpacks the reference SLC files using ISCE readers, also unpackaging the antenna elevation pattern correction file if necessary. This run file also produces the reference geometry files that are consumed downstream.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-ribbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "toolDir=~/topsstack_hamsar/tool\n",
    "\n",
    "source ${toolDir}/00-init.sh\n",
    "source ${toolDir}/01-setup.sh\n",
    "\n",
    "sh ${toolDir}/06-run-01.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuous-tunisia",
   "metadata": {},
   "source": [
    "### Unpack secondary SLC\n",
    "\n",
    "Unpack Secondary SLCs\n",
    "In a manner similar to the SLCs in the step above, this step unpacks the secondary SLCs from each of the input SLC zip files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-estate",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "toolDir=~/topsstack_hamsar/tool\n",
    "\n",
    "source ${toolDir}/00-init.sh\n",
    "source ${toolDir}/01-setup.sh\n",
    "\n",
    "sh ${toolDir}/06-run-02-0.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-electronics",
   "metadata": {},
   "source": [
    "### SLC noise calibration\n",
    "\n",
    "This step runs radiometric and thermal noise calibration using vh-pol data against the SLCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-dominican",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "toolDir=~/topsstack_hamsar/tool\n",
    "\n",
    "source ${toolDir}/00-init.sh\n",
    "source ${toolDir}/01-setup.sh\n",
    "\n",
    "sh ${toolDir}/06-run-02-5.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-apartment",
   "metadata": {},
   "source": [
    "### Discard subswaths not to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-arthritis",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "toolDir=~/topsstack_hamsar/tool\n",
    "\n",
    "source ${toolDir}/00-init.sh\n",
    "source ${toolDir}/01-setup.sh\n",
    "\n",
    "sh ${toolDir}/06-run-02-9.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-calendar",
   "metadata": {},
   "source": [
    "### Average baseline\n",
    "\n",
    "Computes average baseline for the stack. These baselines are not used for processing anywhere. They are only an approximation and can be used for plotting purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-husband",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "toolDir=~/topsstack_hamsar/tool\n",
    "\n",
    "source ${toolDir}/00-init.sh\n",
    "source ${toolDir}/01-setup.sh\n",
    "\n",
    "sh ${toolDir}/06-run-03.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-plate",
   "metadata": {},
   "source": [
    "### fullBurst geo2rdr\n",
    "\n",
    "This step estimates geometrical offsets between secondary burst overlaps and the stack reference burst overlaps. The secondaries are then resampled to the stack reference burst overlaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extended-washington",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "toolDir=~/topsstack_hamsar/tool\n",
    "\n",
    "source ${toolDir}/00-init.sh\n",
    "source ${toolDir}/01-setup.sh\n",
    "\n",
    "sh ${toolDir}/06-run-04.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "running-bunch",
   "metadata": {},
   "source": [
    "### fullBurst resample\n",
    "\n",
    "Using orbit and DEM data, this run file computes geometrical offsets among all secondary SLCs and the stack reference. These offsets, with the misregistration time series are used for precise coregistration of each burst SLC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-queue",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "toolDir=~/topsstack_hamsar/tool\n",
    "\n",
    "source ${toolDir}/00-init.sh\n",
    "source ${toolDir}/01-setup.sh\n",
    "\n",
    "sh ${toolDir}/06-run-05.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-frank",
   "metadata": {},
   "source": [
    "### Extract stack valid region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-nancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "toolDir=~/topsstack_hamsar/tool\n",
    "\n",
    "source ${toolDir}/00-init.sh\n",
    "source ${toolDir}/01-setup.sh\n",
    "\n",
    "sh ${toolDir}/06-run-06.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infectious-allergy",
   "metadata": {},
   "source": [
    "### Merge reference secondary SLC\n",
    "\n",
    "This step merges all bursts for the reference and coregistered SLCs. Geometry files are also merged. Using orbit and DEM, geometrical offsets among all secondary SLCs and the stack reference is computed. The geometrical offsets, together with the misregistration time-series (from previous step) are used for precise coregistration of each burst SLC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-farming",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "toolDir=~/topsstack_hamsar/tool\n",
    "\n",
    "source ${toolDir}/00-init.sh\n",
    "source ${toolDir}/01-setup.sh\n",
    "\n",
    "sh ${toolDir}/06-run-07.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-homeless",
   "metadata": {},
   "source": [
    "### Save result\n",
    "\n",
    "The final step ‘packages’ up the results into a single sub-directory named as coregistered_slcs-<start_date>-<end_date> and generates json files describing the dataset (e.g. the polygon  coordinates of the bounding box) and associated metadata (e.g. included SLCs).\n",
    "\n",
    "More information about metadata files can be found at : https://hysds-core.atlassian.net/wiki/spaces/HYS/pages/389349377/Product+Metadata+Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-carrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "toolDir=~/topsstack_hamsar/tool\n",
    "\n",
    "source ${toolDir}/00-init.sh\n",
    "source ${toolDir}/01-setup.sh\n",
    "\n",
    "prefix=\"_x\"\n",
    "export PROCESSING_START=$(python ${toolDir}/state_rw.py read ${prefix}_processing_start)\n",
    "\n",
    "sh ${toolDir}/07-create-datasets.sh"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
